"""
åšå®¢çŸ¥è¯†åº“å·¥å…·
å°†åšå®¢å†…å®¹è½¬æ¢ä¸ºå‘é‡æ•°æ®åº“ï¼Œä¾›AIæ£€ç´¢ä½¿ç”¨
"""

import os
import json
import hashlib
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime

from langchain_core.tools import tool
try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma

try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.vectorstores.utils import filter_complex_metadata
import markdown
import yaml

from config.config import CONFIG
from utils.logger import logger


class BlogKnowledgeBase:
    """åšå®¢çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, blog_path: str, vector_db_path: str = "data/vector_db"):
        self.blog_path = Path(blog_path)
        self.vector_db_path = Path(vector_db_path)
        self.vector_db_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨æ›´å¥½çš„ä¸­æ–‡æ¨¡å‹
        embedding_model = CONFIG.get("embedding_model", "BAAI/bge-large-zh-v1.5")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}  # åœ¨ç¼–ç æ—¶æ ‡å‡†åŒ–embeddings
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ - ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
        chunk_size = CONFIG.get("chunk_size", 512)
        chunk_overlap = CONFIG.get("chunk_overlap", 100)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]  # æ·»åŠ é€—å·åˆ†éš”ç¬¦
        )
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectorstore = None
        self._load_vectorstore()
        
        # å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        self.metadata_file = self.vector_db_path / "metadata.json"
        self._load_metadata()
    
    def _load_vectorstore(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            if (self.vector_db_path / "chroma.sqlite3").exists():
                self.vectorstore = Chroma(
                    persist_directory=str(self.vector_db_path),
                    embedding_function=self.embeddings
                )
                logger.info("å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
            else:
                logger.info("å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®åº“")
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            self.vectorstore = None
    
    def _load_metadata(self):
        """åŠ è½½å…ƒæ•°æ®"""
        self.metadata = {}
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
                self.metadata = {}
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _parse_markdown_file(self, file_path: Path) -> Dict[str, Any]:
        """è§£æMarkdownæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æfront matter
            front_matter = {}
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    try:
                        front_matter = yaml.safe_load(parts[1]) or {}
                        content = parts[2].strip()
                    except yaml.YAMLError:
                        pass
            
            # è½¬æ¢Markdownä¸ºHTML
            html_content = markdown.markdown(content, extensions=['codehilite', 'tables'])
            
            return {
                'content': content,
                'html_content': html_content,
                'front_matter': front_matter,
                'title': front_matter.get('title', file_path.stem),
                'date': front_matter.get('date', ''),
                'tags': front_matter.get('tags', []),
                'categories': front_matter.get('categories', [])
            }
        except Exception as e:
            logger.error(f"è§£æMarkdownæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {'content': '', 'html_content': '', 'front_matter': {}, 'title': file_path.stem}
    
    def _should_update_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦æ›´æ–°"""
        file_hash = self._get_file_hash(file_path)
        relative_path = str(file_path.relative_to(self.blog_path))
        
        if relative_path not in self.metadata:
            return True
        
        stored_hash = self.metadata[relative_path].get('hash', '')
        return file_hash != stored_hash
    
    def update_knowledge_base(self) -> Dict[str, Any]:
        """æ›´æ–°çŸ¥è¯†åº“"""
        if not self.blog_path.exists():
            return {"success": False, "message": f"åšå®¢è·¯å¾„ä¸å­˜åœ¨: {self.blog_path}"}
        
        logger.info(f"å¼€å§‹æ›´æ–°çŸ¥è¯†åº“ï¼Œåšå®¢è·¯å¾„: {self.blog_path}")
        
        # æ‰«ææ‰€æœ‰Markdownæ–‡ä»¶
        markdown_files = list(self.blog_path.rglob("*.md"))
        logger.info(f"æ‰¾åˆ° {len(markdown_files)} ä¸ªMarkdownæ–‡ä»¶")
        
        updated_files = []
        new_documents = []
        
        for file_path in markdown_files:
            if self._should_update_file(file_path):
                logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
                
                # è§£ææ–‡ä»¶
                parsed_content = self._parse_markdown_file(file_path)
                
                # åˆ†å‰²æ–‡æœ¬
                chunks = self.text_splitter.split_text(parsed_content['content'])
                
                # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                for i, chunk in enumerate(chunks):
                    try:
                        # å¤„ç†å…ƒæ•°æ®ï¼Œå°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯åŸºç¡€ç±»å‹
                        metadata = {
                            'source': str(file_path.relative_to(self.blog_path)),
                            'title': str(parsed_content['title']),
                            'date': str(parsed_content['date']),
                            'tags': ', '.join(str(tag) for tag in parsed_content['tags']) if parsed_content['tags'] else '',
                            'categories': ', '.join(str(cat) for cat in parsed_content['categories']) if parsed_content['categories'] else '',
                            'chunk_index': int(i),
                            'total_chunks': int(len(chunks)),
                            'file_path': str(file_path)
                        }
                        
                        # ç¡®ä¿æ‰€æœ‰å…ƒæ•°æ®å€¼éƒ½æ˜¯ChromaDBæ”¯æŒçš„ç±»å‹
                        filtered_metadata = {}
                        for key, value in metadata.items():
                            if value is None:
                                continue
                            elif isinstance(value, (str, int, float, bool)):
                                filtered_metadata[key] = value
                            else:
                                filtered_metadata[key] = str(value)
                        
                        # å¢å¼ºé¡µé¢å†…å®¹ï¼šå°†æ ‡é¢˜ã€æ ‡ç­¾ã€åˆ†ç±»ä¿¡æ¯æ·»åŠ åˆ°å†…å®¹å¼€å¤´ï¼Œæé«˜æœç´¢ç›¸å…³æ€§
                        enhanced_content = f"æ ‡é¢˜: {parsed_content['title']}\n"
                        if parsed_content['tags']:
                            enhanced_content += f"æ ‡ç­¾: {', '.join(str(tag) for tag in parsed_content['tags'])}\n"
                        if parsed_content['categories']:
                            enhanced_content += f"åˆ†ç±»: {', '.join(str(cat) for cat in parsed_content['categories'])}\n"
                        enhanced_content += f"\n{chunk}"
                        
                        doc = Document(
                            page_content=enhanced_content,
                            metadata=filtered_metadata
                        )
                        new_documents.append(doc)
                        
                    except Exception as e:
                        logger.error(f"åˆ›å»ºæ–‡æ¡£å¤±è´¥ {file_path} chunk {i}: {e}")
                        continue
                
                # æ›´æ–°å…ƒæ•°æ®
                relative_path = str(file_path.relative_to(self.blog_path))
                self.metadata[relative_path] = {
                    'hash': self._get_file_hash(file_path),
                    'last_updated': datetime.now().isoformat(),
                    'title': parsed_content['title'],
                    'chunks_count': len(chunks)
                }
                
                updated_files.append(relative_path)
        
        if new_documents:
            # åˆ›å»ºæˆ–æ›´æ–°å‘é‡æ•°æ®åº“
            if self.vectorstore is None:
                self.vectorstore = Chroma.from_documents(
                    documents=new_documents,
                    embedding=self.embeddings,
                    persist_directory=str(self.vector_db_path)
                )
            else:
                # åˆ é™¤æ—§æ–‡æ¡£å¹¶æ·»åŠ æ–°æ–‡æ¡£
                for file_path in updated_files:
                    try:
                        # åˆ é™¤è¯¥æ–‡ä»¶çš„æ‰€æœ‰chunks
                        collection = self.vectorstore._collection
                        collection.delete(where={"source": file_path})
                        logger.info(f"åˆ é™¤æ—§æ–‡æ¡£: {file_path}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ—§æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
                
                # æ·»åŠ æ–°æ–‡æ¡£
                self.vectorstore.add_documents(new_documents)
            
            # ä¿å­˜å…ƒæ•°æ®
            self._save_metadata()
            
            logger.info(f"çŸ¥è¯†åº“æ›´æ–°å®Œæˆï¼Œå¤„ç†äº† {len(updated_files)} ä¸ªæ–‡ä»¶ï¼Œåˆ›å»ºäº† {len(new_documents)} ä¸ªæ–‡æ¡£å—")
        
        return {
            "success": True,
            "message": f"çŸ¥è¯†åº“æ›´æ–°å®Œæˆ",
            "updated_files": updated_files,
            "new_documents_count": len(new_documents),
            "total_files_processed": len(markdown_files)
        }
    
    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢çŸ¥è¯†åº“"""
        if self.vectorstore is None:
            return []
        
        try:
            # ä½¿ç”¨æ›´å¤§çš„kå€¼è¿›è¡Œåˆæ­¥æœç´¢
            search_k = CONFIG.get("search_k", 10)
            initial_k = max(k * 2, search_k)
            
            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            docs = self.vectorstore.similarity_search_with_score(query, k=initial_k)
            
            results = []
            for doc, score in docs:
                # è®¡ç®—ç»¼åˆè¯„åˆ†
                final_score = self._calculate_relevance_score(query, doc, score)
                
                results.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'score': float(score),
                    'relevance_score': final_score
                })
            
            # æŒ‰ç›¸å…³æ€§å¾—åˆ†é‡æ–°æ’åº
            results.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            # è¿”å›å‰kä¸ªç»“æœ
            return results[:k]
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def _calculate_relevance_score(self, query: str, doc, vector_score: float) -> float:
        """è®¡ç®—æ–‡æ¡£ç›¸å…³æ€§å¾—åˆ†"""
        try:
            content = doc.page_content.lower()
            query_lower = query.lower()
            metadata = doc.metadata
            
            # åŸºç¡€å‘é‡ç›¸ä¼¼æ€§å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºå¾—åˆ†ï¼‰
            base_score = 1.0 / (1.0 + vector_score)
            
            # å…³é”®è¯åŒ¹é…å¾—åˆ†
            keyword_score = 0.0
            query_words = query_lower.split()
            for word in query_words:
                if word in content:
                    keyword_score += 1.0
            keyword_score = keyword_score / len(query_words) if query_words else 0.0
            
            # æ ‡é¢˜åŒ¹é…å¾—åˆ†
            title_score = 0.0
            title = metadata.get('title', '').lower()
            if title and query_lower in title:
                title_score = 2.0  # æ ‡é¢˜åŒ¹é…ç»™æ›´é«˜æƒé‡
            elif title:
                for word in query_words:
                    if word in title:
                        title_score += 0.5
            
            # æ ‡ç­¾åŒ¹é…å¾—åˆ†
            tags_score = 0.0
            tags = metadata.get('tags', '').lower()
            if tags and query_lower in tags:
                tags_score = 1.5
            elif tags:
                for word in query_words:
                    if word in tags:
                        tags_score += 0.3
            
            # åˆ†ç±»åŒ¹é…å¾—åˆ†
            category_score = 0.0
            categories = metadata.get('categories', '').lower()
            if categories and query_lower in categories:
                category_score = 1.0
            elif categories:
                for word in query_words:
                    if word in categories:
                        category_score += 0.2
            
            # ç»¼åˆå¾—åˆ†
            final_score = (
                base_score * 0.4 +           # å‘é‡ç›¸ä¼¼æ€§ 40%
                keyword_score * 0.3 +        # å…³é”®è¯åŒ¹é… 30%
                title_score * 0.15 +         # æ ‡é¢˜åŒ¹é… 15%
                tags_score * 0.1 +           # æ ‡ç­¾åŒ¹é… 10%
                category_score * 0.05        # åˆ†ç±»åŒ¹é… 5%
            )
            
            return final_score
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸å…³æ€§å¾—åˆ†å¤±è´¥: {e}")
            return 1.0 / (1.0 + vector_score)  # fallbackåˆ°åŸºç¡€å¾—åˆ†
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        if self.vectorstore is None:
            return {"total_documents": 0, "total_files": 0}
        
        try:
            # è·å–é›†åˆä¿¡æ¯
            collection = self.vectorstore._collection
            total_docs = collection.count()
            
            return {
                "total_documents": total_docs,
                "total_files": len(self.metadata),
                "vector_db_path": str(self.vector_db_path),
                "blog_path": str(self.blog_path),
                "last_updated": max(
                    [meta.get('last_updated', '') for meta in self.metadata.values()],
                    default=''
                )
            }
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}


# å…¨å±€çŸ¥è¯†åº“å®ä¾‹
_knowledge_base = None

def get_knowledge_base() -> BlogKnowledgeBase:
    """è·å–çŸ¥è¯†åº“å®ä¾‹"""
    global _knowledge_base
    if _knowledge_base is None:
        blog_path = CONFIG.get("blog_path", "/home/zengls/repo/zeroDtree.github.io/content")
        vector_db_path = CONFIG.get("vector_db_path", "data/vector_db")
        _knowledge_base = BlogKnowledgeBase(blog_path, vector_db_path)
    return _knowledge_base


@tool
def update_blog_knowledge_base() -> str:
    """æ›´æ–°åšå®¢çŸ¥è¯†åº“ï¼Œå°†åšå®¢å†…å®¹è½¬æ¢ä¸ºå‘é‡æ•°æ®åº“ä¾›AIæ£€ç´¢ä½¿ç”¨"""
    try:
        kb = get_knowledge_base()
        result = kb.update_knowledge_base()
        
        if result["success"]:
            return f"âœ… çŸ¥è¯†åº“æ›´æ–°æˆåŠŸï¼\n" \
                   f"ğŸ“ å¤„ç†æ–‡ä»¶æ•°: {result['total_files_processed']}\n" \
                   f"ğŸ”„ æ›´æ–°æ–‡ä»¶æ•°: {len(result['updated_files'])}\n" \
                   f"ğŸ“„ æ–°å¢æ–‡æ¡£å—: {result['new_documents_count']}\n" \
                   f"ğŸ“ æ›´æ–°æ–‡ä»¶: {', '.join(result['updated_files'][:5])}" + \
                   (f" ç­‰{len(result['updated_files'])}ä¸ªæ–‡ä»¶" if len(result['updated_files']) > 5 else "")
        else:
            return f"âŒ çŸ¥è¯†åº“æ›´æ–°å¤±è´¥: {result['message']}"
    except Exception as e:
        logger.error(f"æ›´æ–°çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
        return f"âŒ æ›´æ–°çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}"


@tool
def search_blog_knowledge_base(query: str, limit: int = 3) -> str:
    """åœ¨åšå®¢çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼Œé»˜è®¤5ä¸ª
    """
    try:
        kb = get_knowledge_base()
        results = kb.search(query, k=limit)
        
        if not results:
            return f"ğŸ” æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„å†…å®¹"
        
        response = f"ğŸ” æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:\n\n"
        
        for i, result in enumerate(results, 1):
            metadata = result['metadata']
            preview_content = result['content'][:300] + "..." if len(result['content']) > 300 else result['content']
            content = result['content']
            score = result['score']
            relevance_score = result.get('relevance_score', score)
            
            response += f"**{i}. {metadata.get('title', 'æ— æ ‡é¢˜')}**\n"
            response += f"ğŸ“ æ–‡ä»¶: {metadata.get('source', 'æœªçŸ¥')}\n"
            response += f"ğŸ“… æ—¥æœŸ: {metadata.get('date', 'æœªçŸ¥')}\n"
            
            # å¤„ç†æ ‡ç­¾æ˜¾ç¤º
            tags = metadata.get('tags', '')
            if tags:
                response += f"ğŸ·ï¸ æ ‡ç­¾: {tags}\n"
            
            # å¤„ç†åˆ†ç±»æ˜¾ç¤º
            categories = metadata.get('categories', '')
            if categories:
                response += f"ğŸ“‚ åˆ†ç±»: {categories}\n"
            
            response += f"ğŸ“Š å‘é‡ç›¸ä¼¼åº¦: {score:.3f}\n"
            response += f"ğŸ¯ ç»¼åˆç›¸å…³æ€§: {relevance_score:.3f}\n"
            response += f"ğŸ“„ å†…å®¹:\n{content}\n\n"
            response += "---\n\n"
        
        return response
    except Exception as e:
        logger.error(f"æœç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
        return f"âŒ æœç´¢æ—¶å‡ºé”™: {str(e)}"


@tool
def get_blog_knowledge_base_stats() -> str:
    """è·å–åšå®¢çŸ¥è¯†åº“çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        kb = get_knowledge_base()
        stats = kb.get_stats()
        
        if "error" in stats:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats['error']}"
        
        return f"ğŸ“Š åšå®¢çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:\n" \
               f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {stats['total_documents']}\n" \
               f"ğŸ“ æ€»æ–‡ä»¶æ•°: {stats['total_files']}\n" \
               f"ğŸ—‚ï¸ å‘é‡æ•°æ®åº“è·¯å¾„: {stats['vector_db_path']}\n" \
               f"ğŸ“‚ åšå®¢è·¯å¾„: {stats['blog_path']}\n" \
               f"ğŸ•’ æœ€åæ›´æ–°: {stats['last_updated']}"
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"
