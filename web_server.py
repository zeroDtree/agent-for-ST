#!/usr/bin/env python3
"""
WebæœåŠ¡å™¨ - ä¸ºå‰ç«¯æä¾›APIæ¥å£ä¸agentäº¤äº’
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import json
import os

# å¯¼å…¥ç°æœ‰çš„agentç³»ç»Ÿ
from main import create_graph
from states.state import State
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from config.config import CONFIG
from utils.logger import logger
# from utils.preset import preset_messages

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡æ¥å­˜å‚¨agentå›¾
agent_graph = None


def initialize_agent():
    """åˆå§‹åŒ–agentå›¾"""
    global agent_graph
    try:
        agent_graph = create_graph()
        logger.info("Agentç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"Agentç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return False


@app.route("/api/health", methods=["GET"])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({"status": "ok", "agent_ready": agent_graph is not None, "message": "AI Agent WebæœåŠ¡æ­£å¸¸è¿è¡Œ"})


def process_message(messages: list) -> str:
    """å¤„ç†å®Œæ•´å¯¹è¯å†å²å¹¶è¿”å›agentå“åº”"""
    try:
        # å°†OpenAIæ ¼å¼çš„æ¶ˆæ¯è½¬æ¢ä¸ºLangChainæ ¼å¼
        langchain_messages = []

        # æ·»åŠ preset_messagesä½œä¸ºç³»ç»Ÿä¸Šä¸‹æ–‡
        # langchain_messages.extend(preset_messages)

        # è½¬æ¢å¯¹è¯å†å²
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")

            if role == "user":
                langchain_messages.append(HumanMessage(content=content))
            elif role == "assistant":
                langchain_messages.append(AIMessage(content=content))
            elif role == "system":
                langchain_messages.append(SystemMessage(content=content))

        # æ„å»ºè¾“å…¥çŠ¶æ€
        input_state = {
            "messages": langchain_messages,
        }

        # è°ƒç”¨agentå¤„ç†ï¼ˆæ— çŠ¶æ€ï¼‰
        events = agent_graph.stream(
            input=input_state,
            config={
                "recursion_limit": CONFIG["recursion_limit"],
            },
            stream_mode=CONFIG["stream_mode"],
        )

        # æ”¶é›†å“åº”
        response_messages = []
        for event in events:
            if event.get("messages") and len(event["messages"]) > 0:
                response_messages.extend(event["messages"])

        # æå–æœ€åçš„AIå“åº”
        if response_messages:
            last_message = response_messages[-1]
            if isinstance(last_message, AIMessage):
                return last_message.content
            else:
                return str(last_message.content)
        else:
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚"

    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
        raise e


def get_timestamp():
    """è·å–å½“å‰æ—¶é—´æˆ³"""
    from datetime import datetime

    return datetime.now().isoformat()


# ==================== OpenAIå…¼å®¹API ====================


@app.route("/v1/chat/completions", methods=["POST"])
def openai_chat_completions():
    """OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£ï¼Œç”¨äºSillyTaverné›†æˆ"""
    try:
        data = request.get_json()

        # è·å–è¯·æ±‚å‚æ•°
        messages = data.get("messages", [])
        model = data.get("model", "my-agent")
        stream = data.get("stream", False)
        max_tokens = data.get("max_tokens", 2000)
        temperature = data.get("temperature", 0.7)

        if not messages:
            return jsonify({"error": {"message": "messages cannot be empty", "type": "invalid_request_error"}}), 400

        if not agent_graph:
            return jsonify({"error": {"message": "Agent system not initialized", "type": "internal_server_error"}}), 500

        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_message = msg.get("content", "")
                break

        if not user_message:
            return jsonify({"error": {"message": "No user message found", "type": "invalid_request_error"}}), 400

        logger.info(f"OpenAI APIè¯·æ±‚ - æ¨¡å‹: {model}, ç”¨æˆ·æ¶ˆæ¯: {user_message[:100]}...")

        if stream:
            # æµå¼å“åº”
            return handle_streaming_response(model, messages)
        else:
            # éæµå¼å“åº”
            return handle_non_streaming_response(model, messages)

    except Exception as e:
        logger.error(f"å¤„ç†OpenAI APIè¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({"error": {"message": str(e), "type": "internal_server_error"}}), 500


def handle_non_streaming_response(model: str, original_messages: list):
    """å¤„ç†éæµå¼å“åº”"""
    try:
        # å¤„ç†æ¶ˆæ¯
        response_content = process_message(original_messages)

        # æ„å»ºOpenAIå…¼å®¹çš„å“åº”æ ¼å¼
        import uuid

        completion_id = str(uuid.uuid4())[:8]
        response = {
            "id": f"chatcmpl-{completion_id}",
            "object": "chat.completion",
            "created": int(get_timestamp_unix()),
            "model": model,
            "choices": [
                {"index": 0, "message": {"role": "assistant", "content": response_content}, "finish_reason": "stop"}
            ],
            "usage": {
                "prompt_tokens": estimate_tokens(str(original_messages)),
                "completion_tokens": estimate_tokens(response_content),
                "total_tokens": estimate_tokens(str(original_messages)) + estimate_tokens(response_content),
            },
        }

        return jsonify(response)

    except Exception as e:
        logger.error(f"å¤„ç†éæµå¼å“åº”æ—¶å‡ºé”™: {e}")
        return jsonify({"error": {"message": str(e), "type": "internal_server_error"}}), 500


def handle_streaming_response(model: str, messages: list):
    """å¤„ç†æµå¼å“åº”"""
    try:
        from flask import Response
        import uuid
        import json

        def generate():
            try:
                # å°†OpenAIæ ¼å¼çš„æ¶ˆæ¯è½¬æ¢ä¸ºLangChainæ ¼å¼
                langchain_messages = []

                # æ·»åŠ preset_messagesä½œä¸ºç³»ç»Ÿä¸Šä¸‹æ–‡
                # langchain_messages.extend(preset_messages)

                # è½¬æ¢å¯¹è¯å†å²
                for msg in messages:
                    role = msg.get("role", "")
                    content = msg.get("content", "")

                    if role == "user":
                        langchain_messages.append(HumanMessage(content=content))
                    elif role == "assistant":
                        langchain_messages.append(AIMessage(content=content))
                    elif role == "system":
                        langchain_messages.append(SystemMessage(content=content))

                # æ„å»ºè¾“å…¥çŠ¶æ€
                input_state = {
                    "messages": langchain_messages,
                }

                completion_id = str(uuid.uuid4())[:8]

                # å‘é€å¼€å§‹äº‹ä»¶
                start_chunk = {
                    "id": f"chatcmpl-{completion_id}",
                    "object": "chat.completion.chunk",
                    "created": int(get_timestamp_unix()),
                    "model": model,
                    "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}],
                }
                yield f"data: {json.dumps(start_chunk)}\n\n"

                # æµå¼å¤„ç†agentå“åº”
                accumulated_content = ""

                events = agent_graph.stream(
                    input=input_state,
                    config={
                        "recursion_limit": CONFIG["recursion_limit"],
                    },
                    stream_mode="messages",  # ä½¿ç”¨messagesæ¨¡å¼è·å¾—æ›´ç»†ç²’åº¦çš„æµå¼è¾“å‡º
                )

                for message_chunk, metadata in events:
                    if message_chunk.content and metadata["langgraph_node"] not in ["my_tools"]:
                        accumulated_content = message_chunk.content

                        chunk = {
                            "id": f"chatcmpl-{completion_id}",
                            "object": "chat.completion.chunk",
                            "created": int(get_timestamp_unix()),
                            "model": model,
                            "choices": [{"index": 0, "delta": {"content": accumulated_content}, "finish_reason": None}],
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"

                # å‘é€ç»“æŸäº‹ä»¶
                end_chunk = {
                    "id": f"chatcmpl-{completion_id}",
                    "object": "chat.completion.chunk",
                    "created": int(get_timestamp_unix()),
                    "model": model,
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                }
                yield f"data: {json.dumps(end_chunk)}\n\n"

                # å‘é€ç»“æŸæ ‡è®°
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"æµå¼å“åº”ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                # å‘é€é”™è¯¯ä¿¡æ¯
                error_chunk = {
                    "id": f"chatcmpl-error",
                    "object": "chat.completion.chunk",
                    "created": int(get_timestamp_unix()),
                    "model": model,
                    "choices": [
                        {
                            "index": 0,
                            "delta": {"content": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"},
                            "finish_reason": "stop",
                        }
                    ],
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                yield "data: [DONE]\n\n"

        return Response(
            generate(),
            mimetype="text/plain",
            headers={
                "Content-Type": "text/event-stream",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "Content-Type",
            },
        )

    except Exception as e:
        logger.error(f"å¤„ç†æµå¼å“åº”æ—¶å‡ºé”™: {e}")
        return jsonify({"error": {"message": str(e), "type": "internal_server_error"}}), 500


@app.route("/v1/models", methods=["GET"])
def list_models():
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
    return jsonify(
        {
            "object": "list",
            "data": [
                {
                    "id": "my-agent",
                    "object": "model",
                    "created": int(get_timestamp_unix()),
                    "owned_by": "my-agent-system",
                    "permission": [],
                    "root": "my-agent",
                    "parent": None,
                }
            ],
        }
    )


def estimate_tokens(text: str) -> int:
    """ç®€å•çš„tokenæ•°é‡ä¼°ç®—ï¼ˆå¤§çº¦1token=4å­—ç¬¦ï¼‰"""
    if not text:
        return 0
    return max(1, len(text) // 4)


def get_timestamp_unix():
    """è·å–Unixæ—¶é—´æˆ³"""
    from datetime import datetime

    return int(datetime.now().timestamp())


if __name__ == "__main__":
    # åˆå§‹åŒ–agentç³»ç»Ÿ
    if initialize_agent():
        print("ğŸš€ å¯åŠ¨AI Agent APIæœåŠ¡å™¨ (SillyTavernå…¼å®¹)...")
        print("ğŸŒ OpenAI API: http://localhost:5000/v1/chat/completions")
        print("ğŸ“Š å¥åº·æ£€æŸ¥: http://localhost:5000/api/health")

        # å¯åŠ¨æœåŠ¡å™¨
        app.run(host="0.0.0.0", port=5000, debug=True)
    else:
        print("âŒ Agentç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨WebæœåŠ¡å™¨")
